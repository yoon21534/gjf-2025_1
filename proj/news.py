import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import time
from urllib.parse import urljoin, urlparse, parse_qs
import io
import json

class NaverEconomicNewsScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://news.naver.com/'
        }
        self.base_url = "https://news.naver.com"
        
        self.press_political_stance = {
            "ì¡°ì„ ì¼ë³´": "ë³´ìˆ˜", "ì¤‘ì•™ì¼ë³´": "ë³´ìˆ˜", "ë™ì•„ì¼ë³´": "ë³´ìˆ˜",
            "í•œêµ­ê²½ì œ": "ë³´ìˆ˜", "ë§¤ì¼ê²½ì œ": "ë³´ìˆ˜", "ì„œìš¸ê²½ì œ": "ë³´ìˆ˜",
            "í•œê²¨ë ˆ": "ì§„ë³´", "ê²½í–¥ì‹ ë¬¸": "ì§„ë³´", "ì˜¤ë§ˆì´ë‰´ìŠ¤": "ì§„ë³´", "í”„ë ˆì‹œì•ˆ": "ì§„ë³´",
            "ì—°í•©ë‰´ìŠ¤": "ì¤‘ë„", "ë‰´ìŠ¤1": "ì¤‘ë„", "ë‰´ì‹œìŠ¤": "ì¤‘ë„",
            "KBS": "ì¤‘ë„", "MBC": "ì¤‘ë„", "SBS": "ì¤‘ë„", "YTN": "ì¤‘ë„",
            "JTBC": "ì¤‘ë„-ì§„ë³´", "ì±„ë„A": "ë³´ìˆ˜", "TVì¡°ì„ ": "ë³´ìˆ˜", "MBN": "ë³´ìˆ˜",
            "íŒŒì´ë‚¸ì…œë‰´ìŠ¤": "ì¤‘ë„", "ì´ë°ì¼ë¦¬": "ì¤‘ë„", "ì•„ì‹œì•„ê²½ì œ": "ì¤‘ë„",
            "í—¤ëŸ´ë“œê²½ì œ": "ì¤‘ë„", "ë‰´ìŠ¤í•Œ": "ì¤‘ë„", "ë¨¸ë‹ˆíˆ¬ë°ì´": "ì¤‘ë„"
        }
        
    def get_economic_news_list(self, target_date=None, max_pages=1):
        """ë„¤ì´ë²„ ê²½ì œë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° - ë‹¨ìˆœí•˜ê³  ì•ˆì •ì ì¸ ë°©ì‹"""
        if target_date is None:
            target_date = datetime.now()
            
        all_news = []
        
        # ê°€ì¥ ê¸°ë³¸ì ì¸ ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ URLë“¤ë§Œ ì‚¬ìš©
        base_urls = [
            "https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=101",
            "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101"
        ]
        
        for page in range(1, max_pages + 1):
            page_news = []
            
            for base_url in base_urls:
                try:
                    # í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì¶”ê°€
                    if page > 1:
                        url = f"{base_url}&page={page}"
                    else:
                        url = base_url
                    
                    response = requests.get(url, headers=self.headers, timeout=10)
                    
                    if response.status_code != 200:
                        continue
                        
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ê°€ì¥ ì¼ë°˜ì ì¸ ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡° ì…€ë ‰í„°ë“¤
                    selectors_to_try = [
                        'ul.type06_headline li',
                        'ul.type06 li',
                        '.newsflash_body li',
                        '.list_body ul li',
                        'li dt a',
                        'a[href*="news.naver.com/main/read"]'
                    ]
                    
                    articles_found = []
                    for selector in selectors_to_try:
                        elements = soup.select(selector)
                        if elements and len(elements) > 0:
                            articles_found = elements
                            break
                    
                    if not articles_found:
                        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ëª¨ë“  ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘
                        all_links = soup.find_all('a', href=re.compile(r'news\.naver\.com/main/read'))
                        if all_links:
                            articles_found = [link.parent for link in all_links if link.parent]
                    
                    # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                    for element in articles_found[:15]:  # ìµœëŒ€ 15ê°œë§Œ
                        try:
                            article_info = self._extract_simple_article_info(element)
                            if article_info:
                                page_news.append(article_info)
                        except Exception as e:
                            continue
                    
                    if page_news:
                        break
                    
                    time.sleep(1)
                    
                except requests.exceptions.RequestException as e:
                    st.error(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}")
                    continue
                except Exception as e:
                    st.error(f"íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    continue
            
            all_news.extend(page_news)
            time.sleep(2)  # í˜ì´ì§€ ê°„ ë”œë ˆì´
        
        # ì¤‘ë³µ ì œê±°
        unique_news = []
        seen_urls = set()
        for news in all_news:
            if news['link'] not in seen_urls:
                seen_urls.add(news['link'])
                unique_news.append(news)
        
        return unique_news
    
    def _extract_simple_article_info(self, element):
        """ë‹¨ìˆœí•˜ê³  ì•ˆì •ì ì¸ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ë§í¬ ì°¾ê¸°
            link_tag = element.find('a') if element.name != 'a' else element
            if not link_tag or not link_tag.get('href'):
                return None
            
            href = link_tag.get('href')
            
            # URL ì •ê·œí™”
            if href.startswith('//'):
                link = 'https:' + href
            elif href.startswith('/'):
                link = self.base_url + href
            else:
                link = href
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ê²€ì¦
            if 'news.naver.com' not in link:
                return None
            
            # ì œëª© ì¶”ì¶œ - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
            title = ""
            if link_tag.get_text().strip():
                title = link_tag.get_text().strip()
            else:
                # ë¶€ëª¨ ìš”ì†Œì—ì„œ ì œëª© ì°¾ê¸°
                parent = element.parent if element.parent else element
                title_candidates = parent.find_all(['dt', 'h3', 'h4', 'span'])
                for candidate in title_candidates:
                    text = candidate.get_text().strip()
                    if len(text) > 10 and len(text) < 200:
                        title = text
                        break
            
            if not title or len(title) < 5:
                return None
            
            # ì œëª© ì •ë¦¬
            title = re.sub(r'\s+', ' ', title).strip()
            title = re.sub(r'^[^\wê°€-í£]*', '', title)
            
            return {
                'title': title,
                'link': link,
                'press': "ì •ë³´ì—†ìŒ",
                'pub_time': "ì‹œê°„ì •ë³´ì—†ìŒ",
                'political_stance': "ë¯¸ë¶„ë¥˜"
            }
            
        except Exception as e:
            return None
    
    def filter_by_political_stance(self, news_list, allowed_stances):
        if "ì „ì²´" in allowed_stances:
            return news_list
        return [news for news in news_list if news.get('political_stance', 'ë¯¸ë¶„ë¥˜') in allowed_stances]
    
    def filter_by_keyword(self, news_list, keyword):
        if not keyword:
            return news_list
        keyword_lower = keyword.lower()
        return [news for news in news_list if keyword_lower in news['title'].lower()]
    
    def extract_article_content(self, article_url):
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - ì•ˆì •ì ì¸ ë°©ì‹"""
        try:
            if not article_url or 'news.naver.com' not in article_url:
                return self._create_error_result("ì˜ëª»ëœ URL", article_url)
            
            response = requests.get(article_url, headers=self.headers, timeout=15)
            if response.status_code != 200:
                return self._create_error_result(f"HTTP {response.status_code} ì˜¤ë¥˜", article_url)
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë³¸ë¬¸ ì¶”ì¶œ - ê¸°ë³¸ì ì¸ ì…€ë ‰í„°ë“¤ë§Œ ì‚¬ìš©
            content_selectors = [
                '#dic_area',
                '#articleBodyContents',
                '._article_body_contents',
                '.go_trans._article_content'
            ]
            
            content_area = None
            for selector in content_selectors:
                content_area = soup.select_one(selector)
                if content_area:
                    break
            
            if not content_area:
                return self._create_error_result("ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ", article_url)
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in content_area.select('script, style, .ad, .advertisement'):
                tag.decompose()
            
            content = content_area.get_text().strip()
            content = re.sub(r'\n\s*\n', '\n\n', content)
            content = re.sub(r'\s+', ' ', content)
            
            if len(content) < 50:
                return self._create_error_result("ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ", article_url)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            title = self._extract_title(soup)
            press = self._extract_press(soup)
            pub_time = self._extract_time(soup)
            
            return {
                'title': title,
                'content': content,
                'press': press,
                'pub_time': pub_time,
                'url': article_url
            }
            
        except Exception as e:
            return self._create_error_result(f"ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}", article_url)
    
    def _extract_title(self, soup):
        selectors = ['#title_area span', '.media_end_head_headline', 'h2.end_tit', 'h1']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        return "ì œëª© ì—†ìŒ"
    
    def _extract_press(self, soup):
        selectors = ['.media_end_head_top_logo img', '.press_logo img']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get('alt', '')
        return "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ"
    
    def _extract_time(self, soup):
        selectors = ['.media_end_head_info_datestamp_time', '.t11']
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        return "ì‹œê°„ ì •ë³´ ì—†ìŒ"
    
    def _create_error_result(self, error_msg, url):
        return {
            'title': "ì¶”ì¶œ ì‹¤íŒ¨",
            'content': f"ê¸°ì‚¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}",
            'press': "ì˜¤ë¥˜",
            'pub_time': "ì˜¤ë¥˜",
            'url': url
        }
    
    def extract_keywords(self, content, title):
        economic_keywords = [
            'ê¸ˆë¦¬', 'ì¸í”Œë ˆì´ì…˜', 'ë¬¼ê°€', 'GDP', 'ì„±ì¥ë¥ ', 'ì‹¤ì—…ë¥ ', 'í™˜ìœ¨',
            'ì£¼ì‹', 'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ë¶€ë™ì‚°', 'ì•„íŒŒíŠ¸', 'ì „ì„¸', 'ë§¤ë§¤',
            'ìˆ˜ì¶œ', 'ìˆ˜ì…', 'ë¬´ì—­', 'ê²½ìƒìˆ˜ì§€', 'êµ­ì±„', 'íšŒì‚¬ì±„', 'ê¸°ì¤€ê¸ˆë¦¬',
            'í•œêµ­ì€í–‰', 'ê¸°ì—…ì‹¤ì ', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ', 'íˆ¬ì',
            'ì†Œë¹„', 'ë‚´ìˆ˜', 'ê²½ê¸°', 'ì¹¨ì²´', 'íšŒë³µ', 'ìƒìŠ¹', 'í•˜ë½', 'ë°˜ë„ì²´',
            'ìë™ì°¨', 'ì² ê°•', 'ì¡°ì„ ', 'ì„ìœ í™”í•™', 'ë°”ì´ì˜¤', 'IT', 'í†µì‹ '
        ]
        
        text = title + " " + content
        found_keywords = []
        
        for keyword in economic_keywords:
            if keyword in text:
                found_keywords.append(f"#{keyword}")
        
        return found_keywords[:8]
    
    def generate_summary(self, content):
        if not content or len(content) < 50:
            return "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"
            
        sentences = [s.strip() for s in content.split('.') if len(s.strip()) > 20]
        if not sentences:
            return "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"
        
        summary = sentences[0]
        if len(summary) > 150:
            summary = summary[:147] + "..."
        
        return summary

def main():
    st.set_page_config(
        page_title="ğŸ“ˆ ë„¤ì´ë²„ ê²½ì œë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
        page_icon="ğŸ“ˆ",
        layout="wide"
    )
    
    st.title("ğŸ“ˆ ë„¤ì´ë²„ ê²½ì œë‰´ìŠ¤ í¬ë¡¤ëŸ¬")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        selected_date = st.date_input(
            "ğŸ“… ë‚ ì§œ",
            value=datetime.now().date(),
            max_value=datetime.now().date(),
            min_value=datetime.now().date() - timedelta(days=7)
        )
        
        num_articles = st.slider("ğŸ“° ê¸°ì‚¬ ìˆ˜", 1, 10, 3)
        max_pages = st.slider("ğŸ“„ í˜ì´ì§€ ìˆ˜", 1, 3, 1)
        
        political_filter = st.multiselect(
            "ğŸ¯ ì •ì¹˜ì„±í–¥",
            ["ì „ì²´", "ë³´ìˆ˜", "ì¤‘ë„", "ì§„ë³´", "ì¤‘ë„-ì§„ë³´", "ë¯¸ë¶„ë¥˜"],
            default=["ì „ì²´"]
        )
        
        keyword_search = st.text_input("ğŸ” í‚¤ì›Œë“œ", placeholder="ì˜ˆ: ê¸ˆë¦¬, ë°˜ë„ì²´")
        
        st.markdown("---") # ì„¤ì •ê³¼ ë…¸íŠ¸ ì‚¬ì´ êµ¬ë¶„ì„ 
        
        # ë‚˜ì˜ ìŠ¤í¬ë© ë…¸íŠ¸ ì„¹ì…˜ (ì‚¬ì´ë“œë°”ë¡œ ì´ë™)
        st.header("ğŸ“ ë‚˜ì˜ ìŠ¤í¬ë© ë…¸íŠ¸")
        if 'feedback_notes' not in st.session_state:
            st.session_state['feedback_notes'] = {} # í”¼ë“œë°± ë…¸íŠ¸ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            
        if st.session_state['feedback_notes']:
            for url, feedback_data in st.session_state['feedback_notes'].items():
                if feedback_data.get('one_line_review') or \
                   feedback_data.get('unknown_terms') or \
                   feedback_data.get('research_topics'):
                    
                    with st.expander(f"ğŸ“– {feedback_data.get('article_title', 'ì œëª© ì—†ìŒ')}ì— ëŒ€í•œ ë‚˜ì˜ í”¼ë“œë°±", expanded=False):
                        st.markdown(f"**í•œì¤„í‰**: {feedback_data.get('one_line_review', 'ì—†ìŒ')}")
                        st.markdown(f"**ëª¨ë¥´ëŠ” ìš©ì–´**: {feedback_data.get('unknown_terms', 'ì—†ìŒ')}")
                        st.markdown(f"**ë” ì°¾ì•„ë³¸ ê²ƒ**: {feedback_data.get('research_topics', 'ì—†ìŒ')}")
                        st.markdown(f"[ì›ë¬¸ ë³´ê¸°]({url})")
                        st.markdown("---")
        else:
            st.info("ì•„ì§ ì‘ì„±ëœ ë…¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì‚¬ë¥¼ ìŠ¤í¬ë©í•˜ê³  í”¼ë“œë°±ì„ ì‘ì„±í•´ ë³´ì„¸ìš”!")
        
        st.markdown("---") # ë…¸íŠ¸ ì„¹ì…˜ ì•„ë˜ êµ¬ë¶„ì„ 
    
    # ë©”ì¸
    col1, col2 = st.columns([3, 1])
    
    with col1:
        if st.button("ğŸš€ ë‰´ìŠ¤ ìŠ¤í¬ë© ì‹œì‘", type="primary", use_container_width=True):
            scraper = NaverEconomicNewsScraper()
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            target_date = datetime.combine(selected_date, datetime.min.time())
            status_text.text("ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
            
            news_list = scraper.get_economic_news_list(target_date, max_pages)
            
            if not news_list:
                st.error("ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                status_text.empty()
                progress_bar.empty()
                return
            
            # í•„í„°ë§
            filtered_news = scraper.filter_by_political_stance(news_list, political_filter)
            if keyword_search:
                filtered_news = scraper.filter_by_keyword(filtered_news, keyword_search)
            
            if not filtered_news:
                st.warning("ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                status_text.empty()
                progress_bar.empty()
                return
            
            st.success(f"ì´ {len(filtered_news)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            selected_news = filtered_news[:num_articles]
            results = []
            
            # í”¼ë“œë°± ì €ì¥ì„ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”/ë¡œë“œ (ì´ë¯¸ ì‚¬ì´ë“œë°”ì—ì„œ ì´ˆê¸°í™”ë¨)
            # if 'feedback_notes' not in st.session_state:
            #     st.session_state['feedback_notes'] = {} # {article_url: {review, terms, research}}
            
            for i, news in enumerate(selected_news):
                progress = (i + 1) / len(selected_news)
                progress_bar.progress(progress)
                status_text.text(f"[{i+1}/{len(selected_news)}] ê¸°ì‚¬ ì¶”ì¶œ ì¤‘...")
                
                article_data = scraper.extract_article_content(news['link'])
                if article_data:
                    article_data['political_stance'] = news.get('political_stance', 'ë¯¸ë¶„ë¥˜')
                    article_data['keywords'] = scraper.extract_keywords(article_data['content'], article_data['title'])
                    article_data['summary'] = scraper.generate_summary(article_data['content'])
                    results.append(article_data)
                
                time.sleep(1)
            
            status_text.text("ì™„ë£Œ!")
            progress_bar.progress(1.0)
            st.session_state['results'] = results
            st.session_state['scrape_date'] = selected_date
    
    with col2:
        if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", use_container_width=True):
            if 'results' in st.session_state:
                del st.session_state['results']
            # ì„¸ì…˜ ìƒíƒœì˜ í”¼ë“œë°± ë…¸íŠ¸ë„ ì´ˆê¸°í™”
            if 'feedback_notes' in st.session_state:
                del st.session_state['feedback_notes']
            st.rerun()
    
    # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (ì „ì²´ ë‹¤ìš´ë¡œë“œ)
    if 'results' in st.session_state and st.session_state['results']:
        scrape_date = st.session_state.get('scrape_date', datetime.now().date())
        date_str = scrape_date.strftime('%Y-%m-%d') if hasattr(scrape_date, 'strftime') else str(scrape_date)
        
        all_content = f"# ğŸ“ˆ ê²½ì œë‰´ìŠ¤ ìŠ¤í¬ë© ê²°ê³¼\n\n"
        all_content += f"**ë‚ ì§œ**: {date_str}\n"
        all_content += f"**ê¸°ì‚¬ ìˆ˜**: {len(st.session_state['results'])}ê°œ\n\n"
        all_content += "---\n\n"
        
        for i, article in enumerate(st.session_state['results']):
            # í”¼ë“œë°±ì€ ì„¸ì…˜ ìƒíƒœì—ì„œ ê°€ì ¸ì˜¤ê¸°
            feedback_data = st.session_state['feedback_notes'].get(article['url'], {})
            one_line_review_session = feedback_data.get('one_line_review', '')
            unknown_terms_session = feedback_data.get('unknown_terms', '')
            research_topics_session = feedback_data.get('research_topics', '')

            all_content += f"## {i+1}. {article['title']}\n\n"
            all_content += f"- **ì–¸ë¡ ì‚¬**: {article['press']}\n"
            all_content += f"- **ì‹œê°„**: {article['pub_time']}\n"
            all_content += f"- **URL**: {article['url']}\n\n"
            all_content += f"**ìš”ì•½**: {article.get('summary', '')}\n\n"
            all_content += f"**ë³¸ë¬¸**:\n{article['content']}\n\n"
            all_content += f"### ğŸ§  ì…€í”„ í”¼ë“œë°± \n"
            all_content += f"- **í•œì¤„í‰**: {one_line_review_session}\n"
            all_content += f"- **ëª¨ë¥´ëŠ” ìš©ì–´**: {unknown_terms_session}\n"
            all_content += f"- **ë” ì°¾ì•„ë³¸ ê²ƒ**: {research_topics_session}\n\n"
            all_content += "---\n\n"
        
        st.download_button(
            label="ğŸ“¥ ì „ì²´ ë‹¤ìš´ë¡œë“œ",
            data=all_content,
            file_name=f"ê²½ì œë‰´ìŠ¤_{date_str}.md",
            mime="text/markdown"
        )
    
    # ì´ˆê¸° ì‹¤í–‰ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ (ì‚¬ìš©ë²• í¬í•¨)
    if 'results' not in st.session_state:
        st.info("ë‰´ìŠ¤ ìŠ¤í¬ë© ì‹œì‘ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
        
        with st.expander("ğŸ“– ì•± ì‚¬ìš©ë²•", expanded=False):
            st.markdown("""
            ### ğŸ“ ì‚¬ìš© ë°©ë²•
            1. **ë‚ ì§œ ì„ íƒ**: í¬ë¡¤ë§í•  ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”
            2. **ê¸°ì‚¬ ìˆ˜ ì„¤ì •**: 1-10ê°œ ì‚¬ì´ì—ì„œ ì›í•˜ëŠ” ê¸°ì‚¬ ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”
            3. **ì •ì¹˜ì„±í–¥ í•„í„°**: ì›í•˜ëŠ” ì–¸ë¡ ì‚¬ ì„±í–¥ì„ ì„ íƒí•˜ì„¸ìš”
            4. **í‚¤ì›Œë“œ ê²€ìƒ‰**: íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ë³´ê³  ì‹¶ë‹¤ë©´ ì…ë ¥í•˜ì„¸ìš”
            5. **ìŠ¤í¬ë© ì‹œì‘**: ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”
            
            ### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
            - ì–¸ë¡ ì‚¬ë³„ ì •ì¹˜ì„±í–¥ ë¶„ë¥˜
            - í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
            - **ë‚˜ì˜ ìŠ¤í¬ë© ë…¸íŠ¸ **
            - êµ¬ê¸€ ë¬¸ì„œ í˜¸í™˜ Markdown ë‹¤ìš´ë¡œë“œ
            """)
    
    # ê²°ê³¼ í‘œì‹œ
    if 'results' in st.session_state and st.session_state['results']:
        st.markdown("---")
        st.header("ğŸ“„ ê²°ê³¼")
        
        results = st.session_state['results']
        scraper = NaverEconomicNewsScraper()
        
        for i, article_data in enumerate(results):
            # í”¼ë“œë°±ì„ ì„¸ì…˜ ìƒíƒœì—ì„œ ê´€ë¦¬
            # ì´ˆê¸°ê°’ì€ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ê°’ì´ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ë‹¤ë©´ ë¹ˆ ë¬¸ìì—´
            # st.session_state['feedback_notes'] ë”•ì…”ë„ˆë¦¬ì—ì„œ í•´ë‹¹ ê¸°ì‚¬ì˜ í”¼ë“œë°±ì„ ê°€ì ¸ì˜´
            current_feedback = st.session_state['feedback_notes'].get(article_data['url'], {})
            review_default = current_feedback.get('one_line_review', '')
            terms_default = current_feedback.get('unknown_terms', '')
            research_default = current_feedback.get('research_topics', '')

            with st.expander(f"ğŸ“° {i+1}. {article_data['title']}", expanded=(i==0)):
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.markdown(f"**ì–¸ë¡ ì‚¬**: {article_data['press']}")
                with col2:
                    st.markdown(f"**ì‹œê°„**: {article_data['pub_time']}")
                with col3:
                    st.link_button("ğŸ”— ì›ë¬¸", article_data['url'])
                
                st.divider()
                
                # ë¶„ì„
                keywords = article_data.get('keywords', [])
                summary = article_data.get('summary', '')
                
                st.subheader("ğŸ¯ ë¶„ì„")
                st.markdown(f"**ìš”ì•½**: {summary}")
                if keywords:
                    st.markdown(f"**í‚¤ì›Œë“œ**: {' '.join(keywords)}")
                
                st.divider()
                
                # ë³¸ë¬¸
                st.subheader("ğŸ“„ ë³¸ë¬¸")
                st.markdown(article_data['content'])
                
                st.divider()
                
                # ì…€í”„ í”¼ë“œë°± í¼ (ì„¸ì…˜ ì €ì¥ ê¸°ëŠ¥)
                st.subheader("ğŸ§  ì…€í”„ í”¼ë“œë°±")
                with st.form(f"feedback_form_{i}"):
                    one_line_review = st.text_input("í•œì¤„í‰", value=review_default, key=f"review_input_{i}")
                    unknown_terms = st.text_input("ëª¨ë¥´ëŠ” ìš©ì–´", value=terms_default, key=f"terms_input_{i}")
                    research_topics = st.text_input("ë” ì°¾ì•„ë³¸ ê²ƒ", value=research_default, key=f"research_input_{i}")
                    
                    submit_button = st.form_submit_button("ğŸ’¾ ìŠ¤í¬ë© ")

                    if submit_button:
                        # í¼ ì œì¶œ ì‹œ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ (feedback_notes ë”•ì…”ë„ˆë¦¬ì— ì €ì¥)
                        st.session_state['feedback_notes'][article_data['url']] = {
                            'one_line_review': one_line_review,
                            'unknown_terms': unknown_terms,
                            'research_topics': research_topics,
                            'article_title': article_data['title'],
                            'article_url': article_data['url']
                        }
                        st.success("ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # í¼ ì™¸ë¶€ì— ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ë°°ì¹˜
                # ë‹¤ìš´ë¡œë“œ í…œí”Œë¦¿ì— í˜„ì¬ ì„¸ì…˜ì— ì €ì¥ëœ í”¼ë“œë°± ë‚´ìš© ë°˜ì˜
                # st.session_state['feedback_notes']ì—ì„œ ìµœì‹  ë‚´ìš©ì„ ê°€ì ¸ì˜´
                download_feedback = st.session_state['feedback_notes'].get(article_data['url'], {})
                download_template = f"""# ê²½ì œë‰´ìŠ¤ ìŠ¤í¬ë©

## ğŸ“Š ê¸°ë³¸ ì •ë³´
- **ì–¸ë¡ ì‚¬**: {article_data['press']}
- **ê¸°ì‚¬ ì œëª©**: {article_data['title']}
- **ë°œí–‰ì¼ì‹œ**: {article_data['pub_time']}
- **ê¸°ì‚¬ URL**: {article_data['url']}
- **ì¹´í…Œê³ ë¦¬**: ê²½ì œ

## ğŸ¯ í•µì‹¬ ë¶„ì„ 
- **í•œì¤„ ìš”ì•½**: {summary}
- **í‚¤ì›Œë“œ**: {' '.join(keywords)}

## ğŸ“„ ë³¸ë¬¸ 
{article_data['content']}

## ğŸ§  ì…€í”„ í”¼ë“œë°±
- **í•œì¤„í‰**: {download_feedback.get('one_line_review', '')}
- **ëª¨ë¥´ëŠ” ìš©ì–´**: {download_feedback.get('unknown_terms', '')}
- **ë” ì°¾ì•„ë³¸ ê²ƒ**: {download_feedback.get('research_topics', '')}

## ğŸ·ï¸ íƒœê·¸ ì‹œìŠ¤í…œ
`#ì •ì±…` `#ê¸ˆë¦¬` `#ì£¼ì‹` `#ë¶€ë™ì‚°` `#í™˜ìœ¨` `#ê¸°ì—…ì‹¤ì ` `#êµ­ì œê²½ì œ`

---
*ìŠ¤í¬ë© ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
                st.download_button(
                    label=f"ğŸ“¥ ê¸°ì‚¬ {i+1} êµ¬ê¸€ ë¬¸ì„œìš© ê°œë³„ ë‹¤ìš´ë¡œë“œ",
                    data=download_template,
                    file_name=f"ê²½ì œë‰´ìŠ¤_{i+1}_{datetime.now().strftime('%Y%m%d_%H%M')}.md",
                    mime="text/markdown",
                    key=f"download_individual_{i}"
                )

if __name__ == "__main__":
    main()
